{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VandadII/colab-notebooks/blob/main/flux_1_dev_gen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FLUX.1 Gen\n"
      ],
      "metadata": {
        "collapsed": false,
        "id": "zl-S0m3pkQC5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔧 Install required dependencies and create LoRA folder"
      ],
      "metadata": {
        "id": "69Dnd3RrjhSG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade diffusers transformers accelerate torch\n",
        "!pip install protobuf==3.20.3\n",
        "!mkdir -p /content/lora  # Create folder for your LoRA .safetensors file"
      ],
      "metadata": {
        "id": "BvAG0GKAh59G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Put your LoRA in the `/content/lora` folder"
      ],
      "metadata": {
        "id": "UFUW4ZMmnp1V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model License\n",
        "Training currently only works with FLUX.1-dev. Which means anything you train will inherit the non-commercial license. It is also a gated model, so you need to accept the license on HF before using it. Otherwise, this will fail. Here are the required steps to setup a license.\n",
        "\n",
        "Sign into HF and accept the model access here [black-forest-labs/FLUX.1-dev](https://huggingface.co/black-forest-labs/FLUX.1-dev)\n",
        "\n",
        "[Get a READ key from huggingface](https://huggingface.co/settings/tokens/new?) and place it in the next cell after running it."
      ],
      "metadata": {
        "id": "OV0HnOI6o8V6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "# Prompt for the token\n",
        "hf_token = getpass.getpass('Enter your HF access token and press enter: ')\n",
        "\n",
        "# Set the environment variable\n",
        "os.environ['HF_TOKEN'] = hf_token\n",
        "\n",
        "print(\"HF_TOKEN environment variable has been set.\")"
      ],
      "metadata": {
        "id": "3yZZdhFRoj2m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15a8baf6-f04c-4ad5-8d9a-4e7bc6ad6336"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your HF access token and press enter: ··········\n",
            "HF_TOKEN environment variable has been set.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Images with LoRA"
      ],
      "metadata": {
        "id": "N8UUFzVRigbC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from diffusers import FluxPipeline\n",
        "import os\n",
        "\n",
        "# Enable faster downloads from Hugging Face\n",
        "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
        "\n",
        "# Create the output folder if it doesn't exist\n",
        "os.makedirs(\"/content/generated\", exist_ok=True)\n",
        "\n",
        "# Uncomment the following two lines to use a fixed seed for reproducibility\n",
        "# seed = 42  # You can change this value as needed\n",
        "# generator = torch.Generator(device=\"cuda\").manual_seed(seed)\n",
        "\n",
        "# Load the model\n",
        "model_id = \"black-forest-labs/FLUX.1-dev\"\n",
        "pipeline = FluxPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
        "\n",
        "# Load the LoRA weights\n",
        "lora_path = \"/content/lora/your_lora.safetensors\"  # Path to your LoRA file\n",
        "pipeline.load_lora_weights(lora_path)\n",
        "\n",
        "# Move the pipeline to GPU\n",
        "pipeline = pipeline.to(\"cuda:0\")\n",
        "\n",
        "# Define your prompts (10 empty prompts; replace with your own)\n",
        "prompts = [\n",
        "    '',\n",
        "    '',\n",
        "    '',\n",
        "    '',\n",
        "    '',\n",
        "    '',\n",
        "    '',\n",
        "    '',\n",
        "    '',\n",
        "    ''\n",
        "]\n",
        "\n",
        "negative_prompt = \"\"  # Empty, as per your original config\n",
        "\n",
        "# Number of repeats per prompt\n",
        "num_repeats = 4  # From your original config\n",
        "\n",
        "# Generation settings from your original config\n",
        "guidance_scale = 7\n",
        "num_inference_steps = 100  # Your 'sample_steps'\n",
        "\n",
        "# Generate images for each non-empty prompt\n",
        "image_counter = 1\n",
        "for prompt_idx, prompt in enumerate(prompts):\n",
        "    if prompt.strip() == \"\":\n",
        "        continue  # Skip empty prompts\n",
        "    for repeat in range(num_repeats):\n",
        "        print(f\"Generating image {image_counter} (Prompt {prompt_idx+1}, Repeat {repeat+1}): {prompt}\")\n",
        "        image = pipeline(\n",
        "            prompt,\n",
        "            negative_prompt=negative_prompt,\n",
        "            height=1024,\n",
        "            width=1024,\n",
        "            guidance_scale=guidance_scale,\n",
        "            num_inference_steps=num_inference_steps\n",
        "            # Uncomment the following line if you enable the fixed seed above\n",
        "            # generator=generator\n",
        "        ).images[0]\n",
        "        output_path = f\"/content/generated/ferrah_image_{image_counter}.png\"  # No seed in filename since it's random\n",
        "        image.save(output_path)\n",
        "        print(f\"Image {image_counter} saved to {output_path}\")\n",
        "        image_counter += 1"
      ],
      "metadata": {
        "id": "_t28QURYjRQO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}